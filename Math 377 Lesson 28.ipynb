{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "from math import *\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 28: Method of Moments Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue this block with a brief look into estimators and their properties. Suppose we are presented with a random sample of observations and we are interested in the true underlying process from which these observations originated. If we know, or assume, the underlying distribution of these data, we can use the sample to estimate the parameters of that distribution. \n",
    "\n",
    "## Single Parameter \n",
    "\n",
    "Specifically, let $x_1,x_2,...,x_n$ be an iid (independent, identically distributed) sample from a distribution with a single parameter $\\theta$. Typically, the expected value (or average) of a random variable with this distribution can be expressed as a function of $\\theta$. We also know that the sample mean, $\\bar{X}$ is our best guess at true mean, given our data. So we simply set $E(X)$ equal to our sample mean and solve for $\\theta$. The result is the method of moments estimate of $\\theta$, and we write it as $\\hat{\\theta}_{MoM}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Exponential Distribution\n",
    "\n",
    "Suppose $x_1,x_2,...,x_n$ is a random sample of size $n$ from the exponential distribution with unknown parameter $\\lambda$. I would like to obtain $\\hat{\\lambda}_{MoM}$, the method of moments estimate of $\\lambda$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From last block, I know that if $X\\sim \\textsf{Exp}(\\lambda)$, then $E(X)=\\frac{1}{\\lambda}$. In other words, our true population average is $1 \\over \\lambda$. Our best guess of our average given our data is simply the sample mean $\\bar{X}$. So, ideally, $1 \\over \\lambda$ should be close to $\\bar{X}$. We set these two equal to each other and solve for $\\lambda$:\n",
    "\n",
    "$$\n",
    "{1 \\over \\lambda} = \\bar{X}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda}_{MoM} = {1 \\over \\bar{X}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Uniform Distribution\n",
    "\n",
    "Suppose $x_1,x_2,...,x_n$ is a random sample of size $n$ from the uniform distribution on the domain $0\\leq X \\leq b$. In other words, $X\\sim \\textsf{Unif}(0,b)$. Find $\\hat{b}_{MoM}$, the method of moments estimate of b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a uniform distribution from $0$ to $b$, we know that $E(X) = \\frac{b}{2}$\n",
    "\n",
    "$$\n",
    "\\frac{\\sum{x}}{n} = \\frac{b}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to solve for $b$, we have:\n",
    "$$\n",
    "b = \\frac{2\\sum{X}}{n}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I obtain the following values in my sample: $(0.2,0.4,0.3,0.9,0.4)$. What is our estimate of $b$? What is wrong with that estimate? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8800000000000001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = np.array([0.2, 0.4, 0.3, 0.9, 0.4])\n",
    "b = 2*np.mean(vals)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found the upper bound, $b = 0.88$. We know this is wrong because one of the values in the sample is greater than the upper bound we found for this sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Parameters\n",
    "\n",
    "We can extend this to more than one parameter. Suppose our sample comes from a distribution with multiple parameters. We would simply use the higher moments and solve the resulting system of equations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Normal Distribution\n",
    "\n",
    "Suppose $x_1,x_2,...,x_n$ is a random sample of size $n$ from the normal distribution with unknown parameters $\\mu$ and $\\sigma$. I would like to obtain $\\hat{\\mu}_{MoM}$ and $\\hat{\\sigma}_{MoM}$. \n",
    "\n",
    "This is fairly straight forward, since $\\mu$ and $\\sigma$ are directly interpreted as the mean and standard deviation. Specifically, if $X\\sim \\textsf{Norm}(\\mu,\\sigma)$, then $E(X)=\\mu$. Thus, $\\hat{\\mu}_{MoM} = \\bar{X}$. Further, $Var(X)=\\sigma^2$, so we can set this equal to the second sample moment around the mean: \n",
    "$$\n",
    "Var(X)=\\sigma^2 \\approx {\\sum (X_i-\\bar{X})^2 \\over n}\n",
    "$$\n",
    "\n",
    "So,\n",
    "$$\n",
    "\\hat{\\sigma}_{MoM} = \\sqrt{{\\sum (X_i-\\bar{X})^2 \\over n}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Gamma Distribution\n",
    "\n",
    "Suppose $x_1,x_2,...,x_n$ is a random sample of size $n$ from the Gamma distribution with unknown parameters $\\alpha$ and $\\lambda$. Find $\\hat{\\alpha}_{MoM}$ and $\\hat{\\lambda}_{MoM}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that if $X\\sim \\textsf{Gamma}(\\alpha,\\lambda)$, $E(X)={\\alpha \\over \\lambda}$ and $Var(X)={\\alpha \\over \\lambda^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still use $\\bar{x}$ as an estimate for $E(X)$. \n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{\\sum{X}}{n} = \\frac{\\alpha}{\\lambda}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Var(X) = \\frac{\\alpha}{\\lambda^2}\n",
    "$$\n",
    "Substituting what we just found,\n",
    "\n",
    "$$\n",
    "\\lambda\\bar{x} = \\frac{\\lambda^2 \\sum_i{(x_i-\\bar{x})^2}}{n}\n",
    "$$\n",
    "\n",
    "And simplified we have...\n",
    "\n",
    "$$\n",
    "\\hat{\\lambda}_{MoM} = \\frac{\\bar{x}n}{\\sum_i{(x_i-\\bar{x})^2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Note that our method of moment estimates are themselves random variables (since they are based on random samples). Thus, each time I obtain a new sample, I will get a new method of moments estimate of my paramter. Over time the average of those estimates should be close to the true value of the paramter. In other words, $E(\\hat{\\theta}_{MoM})$ should equal $\\theta$. If not, the estimate is said to be biased. \n",
    "\n",
    "### Example 5\n",
    "Going back to the normal example, find $E(\\hat{\\mu}_{MoM})$ and $E(\\hat{\\sigma}^2_{MoM})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, recall that if $X\\sim \\textsf{Norm}(\\mu,\\sigma)$, then \n",
    "\n",
    "1. $E(X)=\\mu$ \n",
    "\n",
    "2. $Var(X)=E(X^2)-E(X)^2 = \\sigma^2$, so $E(X^2)=\\sigma^2 + \\mu^2$\n",
    "\n",
    "3. $\\bar{X} \\sim \\textsf{Norm}(\\mu,{\\sigma^2 \\over n})$\n",
    "\n",
    "    a. $E(\\bar{X})=\\mu$\n",
    "\n",
    "    b. $E(\\bar{X}^2)={\\sigma^2 \\over n} + \\mu^2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "E\\left[\\frac{\\sum{(x_i - \\bar{x})^2}}{n}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. \n",
    "$$\n",
    "E(\\hat\\mu_{MoM}) = E(\\bar{X}) = \\mu\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Through lots of math and a long process of trying to simplify $\\sqrt{\\frac{\\sum(X_i - \\bar{X})^2}{n}}$, I found (with help from Jordan Armstrong) that it would not simplify to $E(\\mu^2)$, so it is a biased estimator."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
